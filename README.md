# AWS ETL Redshift Data Pipeline üöÄ

A complete end-to-end data engineering project implementing a modern data pipeline with AWS services, demonstrating ETL best practices, data quality validation, and automated orchestration.

## üìä Project Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Source    ‚îÇ      ‚îÇ    S3    ‚îÇ      ‚îÇ  Glue ETL    ‚îÇ      ‚îÇ     S3      ‚îÇ
‚îÇ  CSV Files  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Raw)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (PySpark)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Curated)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                    ‚îÇ                     ‚îÇ
                           ‚îÇ                    ‚îÇ                     ‚îÇ
                           ‚ñº                    ‚ñº                     ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Glue Crawler ‚îÇ    ‚îÇ  Data Quality‚îÇ     ‚îÇ   Redshift   ‚îÇ
                    ‚îÇ   (Schema)   ‚îÇ    ‚îÇ    Tests     ‚îÇ     ‚îÇ(Star Schema) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                      ‚îÇ
                                                                      ‚ñº
                                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                              ‚îÇ   Power BI   ‚îÇ
                                                              ‚îÇ  Dashboards  ‚îÇ
                                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pipeline Flow

1. **Raw Data Ingestion**: CSV files uploaded to S3 raw bucket
2. **Schema Discovery**: AWS Glue Crawler catalogs raw data
3. **ETL Transformation**: PySpark job transforms raw ‚Üí curated (star schema)
4. **Data Quality**: Pandera/Great Expectations validation
5. **Data Warehouse**: Redshift COPY loads curated data
6. **Orchestration**: Airflow DAG manages entire pipeline
7. **Visualization**: Power BI connects to Redshift for analytics

## üèóÔ∏è Project Structure

```
AWS-ETL-redshift-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ etl_glue_redshift_dag.py          # Airflow orchestration DAG
‚îÇ
‚îú‚îÄ‚îÄ glue_jobs/
‚îÇ   ‚îî‚îÄ‚îÄ raw_to_curated.py                 # PySpark ETL transformation
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ create_redshift_schema.sql        # Star schema DDL
‚îÇ   ‚îî‚îÄ‚îÄ load_redshift_tables.sql          # COPY commands & validation
‚îÇ
‚îú‚îÄ‚îÄ data_samples/
‚îÇ   ‚îú‚îÄ‚îÄ customers.csv                     # Sample customer data
‚îÇ   ‚îú‚îÄ‚îÄ products.csv                      # Sample product data
‚îÇ   ‚îú‚îÄ‚îÄ locations.csv                     # Sample location data
‚îÇ   ‚îî‚îÄ‚îÄ sales.csv                         # Sample sales transactions
‚îÇ
‚îú‚îÄ‚îÄ powerbi/
‚îÇ   ‚îî‚îÄ‚îÄ README.md                         # Power BI setup guide
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ pandera_tests.py                  # Data quality validation
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ ci.yml                            # GitHub Actions CI/CD
‚îÇ
‚îú‚îÄ‚îÄ README.md                             # This file
‚îî‚îÄ‚îÄ requirements.txt                      # Python dependencies
```

## üéØ Features

### ‚úÖ Complete ETL Pipeline
- **Extract**: Read CSV files from S3
- **Transform**: PySpark data cleaning, validation, and transformation
- **Load**: Load to Redshift star schema

### ‚úÖ Data Quality Assurance
- **Pandera** schemas for validation
- **Great Expectations** data quality tests
- Cross-table referential integrity checks
- Automated data profiling

### ‚úÖ Star Schema Design
- **Fact Table**: `sales_fact` (grain: one sale transaction)
- **Dimensions**: `customer_dim`, `product_dim`, `location_dim`, `date_dim`
- **Materialized Views**: Pre-aggregated analytics queries
- **Optimizations**: Distribution keys, sort keys, compression

### ‚úÖ Orchestration
- **Airflow DAG** with task dependencies
- Error handling and retries
- Validation checkpoints
- Monitoring and logging

### ‚úÖ CI/CD Pipeline
- **GitHub Actions** workflows
- Code quality checks (Black, Flake8, Pylint)
- Security scanning (Bandit, Safety)
- SQL validation (SQLFluff)
- Automated testing

### ‚úÖ Cloud-Native AWS Architecture
- **S3**: Data lake storage
- **Glue**: Serverless ETL & catalog
- **Redshift**: Data warehouse
- **IAM**: Security & permissions
- **CloudWatch**: Monitoring (optional)

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- AWS Account with permissions for S3, Glue, Redshift, IAM
- Airflow 2.7+ (local or AWS MWAA)
- Power BI Desktop (optional)

### 1. Clone Repository

```bash
git clone https://github.com/YOUR_USERNAME/AWS-ETL-redshift-pipeline.git
cd AWS-ETL-redshift-pipeline
```

### 2. Install Dependencies

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install packages
pip install -r requirements.txt
```

### 3. Configure AWS Credentials

```bash
aws configure
# Enter your AWS Access Key ID, Secret Access Key, Region
```

### 4. Set Up S3 Buckets

```bash
# Create buckets
aws s3 mb s3://your-raw-data-bucket --region us-east-1
aws s3 mb s3://your-curated-data-bucket --region us-east-1

# Upload sample data
aws s3 cp data_samples/ s3://your-raw-data-bucket/ --recursive
```

### 5. Create Glue Database

```bash
aws glue create-database \
    --database-input '{"Name": "sales_db", "Description": "Sales data catalog"}'
```

### 6. Configure Glue Crawler

```bash
aws glue create-crawler \
    --name raw-data-crawler \
    --role arn:aws:iam::YOUR_ACCOUNT:role/GlueServiceRole \
    --database-name sales_db \
    --targets '{"S3Targets": [{"Path": "s3://your-raw-data-bucket/"}]}'
```

### 7. Create Glue ETL Job

```bash
# Upload Glue script to S3
aws s3 cp glue_jobs/raw_to_curated.py s3://your-curated-data-bucket/scripts/

# Create Glue job
aws glue create-job \
    --name raw-to-curated-etl \
    --role arn:aws:iam::YOUR_ACCOUNT:role/GlueServiceRole \
    --command '{"Name": "glueetl", "ScriptLocation": "s3://your-curated-data-bucket/scripts/raw_to_curated.py", "PythonVersion": "3"}' \
    --glue-version "4.0" \
    --worker-type "G.1X" \
    --number-of-workers 10
```

### 8. Set Up Redshift Cluster

```bash
# Create Redshift cluster (example)
aws redshift create-cluster \
    --cluster-identifier sales-dw \
    --node-type dc2.large \
    --master-username admin \
    --master-user-password YourPassword123! \
    --cluster-type single-node \
    --db-name salesdb
```

Wait for cluster to be available (~5-10 minutes).

### 9. Create Redshift Schema

```bash
# Connect to Redshift and run SQL
psql -h your-cluster.region.redshift.amazonaws.com -U admin -d salesdb -p 5439 -f sql/create_redshift_schema.sql
```

### 10. Configure Airflow

```bash
# Set Airflow home
export AIRFLOW_HOME=~/airflow

# Initialize Airflow database
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# Copy DAG
cp dags/etl_glue_redshift_dag.py $AIRFLOW_HOME/dags/

# Configure connections
airflow connections add aws_default \
    --conn-type aws \
    --conn-extra '{"region_name": "us-east-1"}'

airflow connections add redshift_default \
    --conn-type postgres \
    --conn-host your-cluster.region.redshift.amazonaws.com \
    --conn-schema salesdb \
    --conn-login admin \
    --conn-password YourPassword123! \
    --conn-port 5439

# Start Airflow
airflow webserver --port 8080 &
airflow scheduler &
```

### 11. Run Pipeline

**Option A: Via Airflow UI**
1. Open http://localhost:8080
2. Enable `aws_glue_redshift_etl_pipeline` DAG
3. Trigger DAG manually

**Option B: Via CLI**
```bash
airflow dags trigger aws_glue_redshift_etl_pipeline
```

### 12. Verify Data in Redshift

```sql
-- Check row counts
SELECT 'customer_dim' AS table_name, COUNT(*) FROM customer_dim
UNION ALL
SELECT 'product_dim', COUNT(*) FROM product_dim
UNION ALL
SELECT 'location_dim', COUNT(*) FROM location_dim
UNION ALL
SELECT 'date_dim', COUNT(*) FROM date_dim
UNION ALL
SELECT 'sales_fact', COUNT(*) FROM sales_fact;

-- Sample analytics query
SELECT 
    d.year,
    d.month_name,
    p.category,
    SUM(s.net_amount) AS total_sales
FROM sales_fact s
JOIN date_dim d ON s.date_key = d.date_key
JOIN product_dim p ON s.product_key = p.product_key
GROUP BY d.year, d.month_name, p.category
ORDER BY d.year, d.month, total_sales DESC;
```

## üìà Data Model

### Star Schema Design

#### Fact Table: `sales_fact`
- **Grain**: One row per sale transaction
- **Measures**: quantity, unit_price, gross_amount, discount_amount, tax_amount, net_amount
- **Foreign Keys**: customer_key, product_key, location_key, date_key

#### Dimension Tables

**`customer_dim`** (SCD Type 1)
- customer_key (PK), customer_id, full_name, email, phone, address, city, state, country

**`product_dim`** (SCD Type 1)
- product_key (PK), product_id, product_name, category, subcategory, brand, unit_price, cost, margin

**`location_dim`**
- location_key (PK), location_id, store_name, address, city, state, region, country

**`date_dim`**
- date_key (PK), date_value, year, quarter, month, day, day_of_week, is_weekend, fiscal_year

### Materialized Views

**`mv_monthly_sales`**
- Pre-aggregated sales by year, month, category, region

**`mv_customer_summary`**
- Customer lifetime value, purchase frequency, recency

## üß™ Data Quality Tests

### Run Tests Locally

```bash
cd tests
python pandera_tests.py
```

### Test Coverage

‚úÖ **Schema Validation**
- Column types and names
- Required fields (NOT NULL)
- Value ranges and constraints

‚úÖ **Data Quality Rules**
- Email format validation
- ID format validation (CUST\d+, PROD\d+)
- Price >= 0
- Quantity > 0

‚úÖ **Referential Integrity**
- All customer_ids in sales exist in customer table
- All product_ids in sales exist in product table
- All location_ids in sales exist in location table

‚úÖ **Business Rules**
- gross_amount = quantity * unit_price
- net_amount = gross_amount - discount + tax
- No duplicate primary keys

## üîÑ CI/CD Pipeline

GitHub Actions workflow automatically runs on every push:

1. **Code Quality**: Black, Flake8, Pylint, isort
2. **Security Scan**: Bandit, Safety, Trivy
3. **Data Validation**: Pandera tests
4. **SQL Validation**: SQLFluff linting
5. **Unit Tests**: Pytest with coverage
6. **Build & Package**: Create deployment artifacts
7. **Deploy to Dev**: Auto-deploy on main branch

### Trigger CI/CD

```bash
git add .
git commit -m "Update ETL pipeline"
git push origin main
```

View results at: https://github.com/YOUR_USERNAME/AWS-ETL-redshift-pipeline/actions

## üìä Power BI Dashboard

See [powerbi/README.md](powerbi/README.md) for detailed setup instructions.

### Quick Connect

1. Open Power BI Desktop
2. Get Data ‚Üí Amazon Redshift
3. Enter Redshift endpoint and database
4. Import tables: `sales_fact`, `customer_dim`, `product_dim`, `location_dim`, `date_dim`
5. Create relationships (star schema)
6. Build visualizations

### Sample Dashboards

- **Executive Summary**: KPIs, trends, top products
- **Product Analytics**: Category performance, pricing analysis
- **Customer Analytics**: Lifetime value, segmentation, RFM
- **Geographic Analysis**: Regional sales, store performance
- **Time Intelligence**: YoY growth, seasonality, forecasts

## üõ†Ô∏è Troubleshooting

### Common Issues

**1. Glue Job Fails**
```
Error: Table not found in Glue Catalog
Solution: Run Glue Crawler first to catalog raw data
```

**2. Redshift Connection Timeout**
```
Error: Cannot connect to Redshift
Solution: 
- Check security group allows inbound on port 5439
- Verify cluster is publicly accessible (if connecting from local)
- Check VPC and subnet settings
```

**3. Airflow DAG Not Appearing**
```
Issue: DAG not showing in UI
Solution:
- Check DAG file has no Python syntax errors
- Verify file is in $AIRFLOW_HOME/dags/
- Check Airflow logs: airflow dags list-import-errors
```

**4. Data Quality Test Failures**
```
Issue: Pandera validation fails
Solution:
- Check CSV data format matches schema
- Verify date formats are consistent
- Ensure no duplicate IDs in dimension tables
```

### Useful Commands

```bash
# Check Glue Crawler status
aws glue get-crawler --name raw-data-crawler

# Check Glue Job runs
aws glue get-job-runs --job-name raw-to-curated-etl --max-results 5

# Query Redshift
psql -h your-cluster.region.redshift.amazonaws.com -U admin -d salesdb -p 5439

# View Airflow logs
airflow tasks logs aws_glue_redshift_etl_pipeline task_name 2024-01-01

# Test data quality
python tests/pandera_tests.py
```

## üìö Technologies Used

| Category | Technologies |
|----------|-------------|
| **Cloud Platform** | AWS (S3, Glue, Redshift, IAM) |
| **Data Processing** | PySpark, Pandas, NumPy |
| **Orchestration** | Apache Airflow |
| **Data Quality** | Pandera, Great Expectations |
| **Data Warehouse** | Amazon Redshift |
| **Visualization** | Power BI |
| **CI/CD** | GitHub Actions |
| **Languages** | Python, SQL |
| **Testing** | Pytest, Moto |
| **Code Quality** | Black, Flake8, Pylint, Bandit |

## üéì Learning Outcomes

This project demonstrates:

‚úÖ **Data Engineering Best Practices**
- Star schema design
- SCD Type 1 dimensions
- Incremental loading patterns
- Data quality validation

‚úÖ **Cloud Architecture**
- Serverless ETL with AWS Glue
- S3 data lake patterns
- Redshift optimization (dist keys, sort keys)
- IAM security

‚úÖ **DevOps & DataOps**
- CI/CD automation
- Infrastructure as Code (IaC)
- Monitoring and logging
- Version control

‚úÖ **Production-Ready Code**
- Error handling and retries
- Logging and observability
- Documentation
- Testing and validation

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## üìß Contact

Your Name - your.email@example.com

Project Link: https://github.com/YOUR_USERNAME/AWS-ETL-redshift-pipeline

## üôè Acknowledgments

- AWS Documentation
- Apache Airflow Community
- Pandera & Great Expectations Teams
- Data Engineering Community

---

## üìù Next Steps

To extend this project:

1. **Add Incremental Loading**: Implement CDC (Change Data Capture)
2. **Add Data Versioning**: Integrate with Delta Lake or Apache Iceberg
3. **Implement SCD Type 2**: Track historical changes in dimensions
4. **Add Real-time Streaming**: Use Kinesis or Kafka for real-time data
5. **Implement Data Lineage**: Use tools like Amundsen or DataHub
6. **Add ML Pipeline**: Integrate SageMaker for predictive analytics
7. **Monitoring Dashboard**: Create CloudWatch/Grafana dashboards
8. **Cost Optimization**: Implement S3 lifecycle policies, Redshift auto-pause

## üìä Performance Benchmarks

| Operation | Duration | Records |
|-----------|----------|---------|
| Glue Crawler | ~2 min | 4 tables |
| Glue ETL Job | ~5 min | 100K rows |
| Redshift COPY | ~30 sec | 100K rows |
| Full Pipeline | ~10 min | End-to-end |

---

**‚≠ê If you find this project helpful, please star the repository!**

